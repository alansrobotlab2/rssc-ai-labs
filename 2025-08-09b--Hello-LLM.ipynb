{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2ada39",
   "metadata": {},
   "source": [
    "## <span style=\"color: rgb(137,207,240)\">Introduction to LLMs</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee87c2d",
   "metadata": {},
   "source": [
    "#### <span style=\"color: rgb(137,207,240)\">Fundamentals</span>\n",
    "First things first, we need to import the package that allows us to use the large language model.  For today's lab we're going to either use our llm that's hosted on ollama locally or through openai.\n",
    "\n",
    "If you'd like, take a look at pyproject.toml to see where we add ChatOpenAI to our python virtual environment.\n",
    "\n",
    "We're using a package from an AI framework called langgraph. For more information about this class see:\n",
    "https://python.langchain.com/docs/integrations/chat/openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de57ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08119950",
   "metadata": {},
   "source": [
    "Now, we need to declare in instance of the ChatOpenAI class.  We also pass in a couple of housekeeping values, like the endpoint, the model we need etc.\n",
    "\n",
    "You'll notice that OPENAI_KEY has a squiggly line, meaning it's undefined..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba4586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    #endpoint = \"http://localhost:11434/v1/chat/completions\",  # Ollama endpoint\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    api_key = \"YOUR_API_KEY_HERE\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09146c0",
   "metadata": {},
   "source": [
    "1. Now let's use it.  our instance of the llm class has a method called .invoke.  It passes the text to the language model and returns the response.  create a code cell below that calls the following method.  `llm.invoke(\"something something\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df29a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f07026",
   "metadata": {},
   "source": [
    "2. Now let's create a code cell below with your own question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48218c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc235a04",
   "metadata": {},
   "source": [
    "3. Ok, now that we've asked a few questions, let's tell the model something.  invoke the llm with a statement, like \"my name is...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf9843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b78c07b",
   "metadata": {},
   "source": [
    "4. And let's ask the model what my name is to see if it remembers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b810f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "601e0ad5",
   "metadata": {},
   "source": [
    "5. Hmm, that didn't go as expected.  This is because (for the most part) everything you need the model to know is passed to the model every time you call it.  Let's try that again by embedding the information in the invoke statement.  \n",
    "\n",
    "It turns out that llm.invoke() accepts not only a single string for a single interaction, but also a list of messages.  This is actually how chatgpt and other chatbots work.  It uses your conversation history as a form of **memory**, which is added to then passed along to the language model for every interaction.\n",
    "\n",
    "There are 3 types of messages in the conversation history, each with their own purpose:\n",
    "- system:  This is generally the first message in the list, and tells the model important things about what is expected of it.  things like \"you are a helpful ai assistant\"  or \"you're a semi helpful ai that always responds in pirate talk like that spongebob cartoon\" and everything in between.  Let's construct an example message history to see how it works.\n",
    "- user: these are your messages to the assistant\n",
    "- assistant: there are the responses from the assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c57559",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"well hello there!\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"my name is Alan, what is your name?\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"remind me, what is my name?\"})\n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0d4e9",
   "metadata": {},
   "source": [
    "##### <span style=\"color: rgb(137,207,240)\">Activity</span>\n",
    "\n",
    "6. To wrap up this section, copy and paste the code from the cell above, and update the system prompt to give your chatbot some personality.  Keep it PC, because I'm probably going to ask you about it.  ðŸ¤¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9231a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f23d4e50",
   "metadata": {},
   "source": [
    "7. Finally, let's see what the messages list looks like.  Add a code cell below with a print statement that prints out the messages list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67ac20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rssc-ai-labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
